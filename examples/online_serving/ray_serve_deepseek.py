from ray import serve
from ray.serve.llm import LLMConfig, build_openai_app
'\nDeploy DeepSeek R1 or V3 with Ray Serve LLM.\n\nRay Serve LLM is a scalable and production-grade model serving library built\non the Ray distributed computing framework and first-class support for the vLLM engine.\n\nKey features:\n- Automatic scaling, back-pressure, and load balancing across a Ray cluster.\n- Unified multi-node multi-model deployment.\n- Exposes an OpenAI-compatible HTTP API.\n- Multi-LoRA support with shared base models.\n\nRun `python3 ray_serve_deepseek.py` to launch an endpoint.\n\nLearn more in the official Ray Serve LLM documentation:\nhttps://docs.ray.io/en/latest/serve/llm/serving-llms.html\n'
llm_config = LLMConfig(model_loading_config={'model_id': 'deepseek', 'model_source': 'deepseek-ai/DeepSeek-R1'}, deployment_config={'autoscaling_config': {'min_replicas': 1, 'max_replicas': 1}}, accelerator_type='H100', runtime_env={'env_vars': {'VLLM_USE_V1': '1'}}, engine_kwargs={'tensor_parallel_size': 8, 'pipeline_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'dtype': 'auto', 'max_num_seqs': 40, 'max_model_len': 16384, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'trust_remote_code': True})
llm_app = build_openai_app({'llm_configs': [llm_config]})
serve.run(llm_app)